@article{CHANDRASHEKAR201416,
title = {A survey on feature selection methods},
journal = {Computers & Electrical Engineering},
volume = {40},
number = {1},
pages = {16-28},
year = {2014},
note = {40th-year commemorative issue},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2013.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0045790613003066},
author = {Girish Chandrashekar and Ferat Sahin},
abstract = {Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques.}
}

@Manual{shiny,
  title = {shiny: Web Application Framework for R},
  author = {Winston Chang and Joe Cheng and JJ Allaire and Carson Sievert and Barret Schloerke and Yihui Xie and Jeff Allen and Jonathan McPherson and Alan Dipert and Barbara Borges},
  year = {2024},
  note = {R package version 1.8.0.9000, 
https://github.com/rstudio/shiny},
  url = {https://shiny.posit.co/},
}
@article{guyon2003introduction,
  title={An introduction to variable and feature selection},
  author={Guyon, Isabelle and Elisseeff, Andr{\'e}},
  journal={Journal of machine learning research},
  volume={3},
  number={Mar},
  pages={1157--1182},
  year={2003}
}
@article{xu2010discriminative,
  title={Discriminative semi-supervised feature selection via manifold regularization},
  author={Xu, Zenglin and King, Irwin and Lyu, Michael Rung-Tsong and Jin, Rong},
  journal={IEEE Transactions on Neural networks},
  volume={21},
  number={7},
  pages={1033--1047},
  year={2010},
  publisher={IEEE}
}
@article{KOHAVI1997273,
title = {Wrappers for feature subset selection},
journal = {Artificial Intelligence},
volume = {97},
number = {1},
pages = {273-324},
year = {1997},
note = {Relevance},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00043-X},
url = {https://www.sciencedirect.com/science/article/pii/S000437029700043X},
author = {Ron Kohavi and George H. John},
keywords = {Classification, Feature selection, Wrapper, Filter},
abstract = {In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach and show a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes.}
}
@article{battiti1994using,
  title={Using mutual information for selecting features in supervised neural net learning},
  author={Battiti, Roberto},
  journal={IEEE Transactions on neural networks},
  volume={5},
  number={4},
  pages={537--550},
  year={1994},
  publisher={IEEE}
}
@ARTICLE{Kwak2002,
  author={Kwak, N. and Chong-Ho Choi},
  journal={IEEE Transactions on Neural Networks}, 
  title={Input feature selection for classification problems}, 
  year={2002},
  volume={13},
  number={1},
  pages={143-159},
  keywords={Mutual information;Neural networks;Biological neural networks;Principal component analysis;Decision trees;High performance computing;Information analysis;Data mining;Educational technology;Educational programs},
  doi={10.1109/72.977291}}
  @article{peng2005feature,
  title={Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy},
  author={Peng, Hanchuan and Long, Fuhui and Ding, Chris},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={27},
  number={8},
  pages={1226--1238},
  year={2005},
  publisher={IEEE}
}
@article{guyon2002gene,
  title={Gene selection for cancer classification using support vector machines},
  author={Guyon, Isabelle and Weston, Jason and Barnhill, Stephen and Vapnik, Vladimir},
  journal={Machine learning},
  volume={46},
  pages={389--422},
  year={2002},
  publisher={Springer}
}
@article{haury2011influence,
  title={The influence of feature selection methods on accuracy, stability and interpretability of molecular signatures},
  author={Haury, Anne-Claire and Gestraud, Pierre and Vert, Jean-Philippe},
  journal={PloS one},
  volume={6},
  number={12},
  pages={e28210},
  year={2011},
  publisher={Public Library of Science San Francisco, USA}
}
@article{abeel2010robust,
  title={Robust biomarker identification for cancer diagnosis with ensemble feature selection methods},
  author={Abeel, Thomas and Helleputte, Thibault and Van de Peer, Yves and Dupont, Pierre and Saeys, Yvan},
  journal={Bioinformatics},
  volume={26},
  number={3},
  pages={392--398},
  year={2010},
  publisher={Oxford University Press}
}