---
title: "Define the problem"
author: 
  - name: "Nguyễn Đức Phú"
    email: ducphu.1906@gmail.com
    affiliations:
      - name: Ho Chi Minh University of Technology
        city:  Tp. Hồ Chí Minh
        state: Vietnam
execute: 
  echo: false
format: 
  html:
    toc: true 
    toc-title: Table of contents
    toc-location: left

    toc-depth: 3
    lof: true
    lot: true
    
    embed-resources: true

    number-sections: true 
    number-depth: 3

    html-math-method: katex

    code-fold: true
    
    papersize: a4
bibliography: references.bib
---

# Input

What we have:

- 53 datasets of 53 times series data (may be duplicated)
- one LABEL dataset (raw)

```{r}
list.files("dataset/1501")
```

# Output

Following the design of experiment (DOE), we define the outcome and our objectives

## Outcome

Since the LABEL dataset is the result of checking semi-products' quality criterias such as:

```{r}
df <- openxlsx::read.xlsx("dataset/labels.xlsx", sheet = "Sheet2", detectDates = FALSE)
colnames(df)
```

Quality criterias such as `AD`, `Vis`, `pH`, `SG` with quite enough data can be used as dependent variables for the thesis

## Objectives

We have already identified the target. The next task is to state the objectives.

Some options are

- Predict the value of quality critierias, or the `regression` problem
- Predict whether the batch is qualified, or the `binary classification` problem
- Predict which level of `goodness` the batch belongs to, or the `multi-label classification` problem

**Notes**:

1. Search scientific publication for these above terms: `regression`, `binary classification` and `multi-label classification`.
2. Some survey about the amount, advantages, dis-advantages of each type of problem in manufacturing industries.
3. Define levels of 'goodness' in industries and which papers used the same term.

The `Objective` of the thesis is stated below

1. Conduct a feature selection in which the chosen factor are decided by explainable methods such as statistics or SHAP value
2. Modeling the relationship between factors and outcomes to construct a decision - support application in which user will give inputs  and the models show the calculation and recommend the decision to make.
3. Build an web-based software that has a user interface for the decision-support application.

# Approaches

## Statistical and explainable feature selection

For each factors, we conduct analysises in explainable method, namely statistics value like correlation, analysis of variance, SHAP value, etc, to sort the factors by their importances.

## Decision making support model

After the subset of features is determined, the decision making process is modeled via either calculation or machine learning algorithms which are also selected based on their abilities to be interpretable.

Decision making support model is a model in which user are allowed to input the data and set the desired parameter settings, for example, the significance level or threshold that decides whether factors are considered important.

## User interface for decision making tools

An user interface built with library `shiny` [@shiny] and statistical programming language R.

# Literature Review

## Improving performances with statistic-based feature selection

We have total of 53 dataset, over 40 of them are data from machine, which results in many features can be extracted.

To enhance the performance of models, we conduct statistic analysis to determine the impact of features on the outcome defined above

For example, how the average of temperature in the batch has correlation with the pH value.

Regarding the statistic analysis we mentioned, some approaches can be:

- ***Hypothesis testing*** to accept or reject the effect of factors on the outcome
- **Analysis of correlation**  to select features 

According to [@CHANDRASHEKAR201416], there are 3 main types of feature selection methods `Filter`, `Wrapper`, and `Embedded`.

- **Filter** use variable ranking techniques for variable selection by ordering.
- **Wrapper** use predictor as black box and the performances as the objective function to evaluate the variable subset.
- **Embedded** incorporate the feature selection as part of the training process.

### Using Filter

`Hypothesis testing` and `analysis of correlation` use $p\_value$ and $corr$, respectively, as the criteria for ranking. Therefore, both of them belongs to **filter** class.

`Mutual information` (MI) and other methods estimating the MI are also candiates as a principle criteria, such as the measure $K$ of the Kullback-Leibler divergence or Conditional Mutual Information

One of the drawbacks of ranking methods is that they do not eliminate redundant variables in [@guyon2003introduction]. Also, features may be less effective when standing alone but have impact on the target when in form of combination with others in [@guyon2003introduction; @xu2010discriminative].

### Using Wrapper

In [@CHANDRASHEKAR201416], Wrapper can be splitted to two main categories, namely `Sequential Selection Algorithms` and `Heuristic Search Algorithms`.

The main drawbacks for Wrapper methods is that they require computations to select optimal set of features. By using classifiers and their performances as scoring criteria, Wrapper is also vulnerable with model overfitting in [@KOHAVI1997273].

### Using Embedded

Embedded methods want to reduce the computations used by Wrapper by incorporating the feature selection in the training process. Also by combining the ranking criteria as MI and the classificatin models, Embedded methods can limit somes of the cons of Filter approaches.

In [@battiti1994using; @Kwak2002; @peng2005feature], MI is used with other classifiers to acquire the feature subset. Using weights of a classification model as a ranking table to remove feature is also widely used, in [@guyon2003introduction; @guyon2002gene].

### Others

Ensemble feature selection in [@haury2011influence; @abeel2010robust] is a novel candiate that they conduct single feature selection on a bootstrap data sample and aggregate to a final feature set.

## Explainable AI

Artificially intelligence system increasingly affects human, therefore, human demands a clearer explainations from these system before making decisions. 

According to [@app12031353], the concepts of explainable artificial intelligence (XAI) have four main aspects, namely `Stage`, `Scope`, `Input` and `Output`, in [@vilone2021]. 

### Stage

[@vilone2021] states that the `stages` can be defined as *ante hoc* and *post hoc*.

- ***Ante hoc*** starts before the training of data to generate explaination for the decisions. For examples, transparent models such as `fuzzy models` and `tree-based models`
- ***Post hoc*** combines the base model with external model which mimics the base model's behaviour to generate an explaination for the users. For examples, this method is usually associated with models in which the inference process is a blackbox, such as `support vector machines` or `neural networks`.

### Scope

The scope defines the extent of an explaination, globally or locally. Global scope means the whole inferential technique of a model is made transparent to the user. Meanwhile, explaination which is given to user for a single instance of inference is called local scope. The `decision tree` can be an example for both scope. The whole tree represents the global scope, while a single branch ca be treated as a local explaination.

## Web-based application

After all the analysis and modeling, a Shiny Application as demo with [@shiny].

# Modeling

## Data preprocessing

```{r}
library("data.table")
```

```{r}
df <- openxlsx::read.xlsx("dataset/labels.xlsx", sheet = "Sheet2", detectDates = FALSE)
colnames(df)
```
Remove non-infomative features
```{r}
dt <- data.table::data.table(df)
dt <- dt[, .(TUẦN, Ngày.tháng.năm, Ca, Mixer, Code.dầu, KL.theo.mixer, KL.thực.tế, Nước.rework, Batch, Màu, Mùi, AD, Vis, pH, SG)]
```
Since the input data belongs to only mixer 1501, only the label for batches in mixer 1501 should be kept.
```{r}
unique(dt$Mixer)
```
Some unexpected values exist in column Mixer, such as `NA` value. Some possible typos can be seen such as `301`, `1502`, `51`, `504`.

Without the context, it is the best to keep only records with value `1501`.

```{r}
dt1501 <- dt[Mixer == 1501, ]
summary(dt1501)
```

We can seen in the summary report. Numeric labels like AD, pH, SG need to be pre-processed. 

### AD
```{r}
dt1501_ad <- dt1501$AD
length(dt1501_ad[is.numeric(dt1501_ad)])
```
None of current data in column `AD` is numerical. We then try to convert them to numerical ones.

Number of `NaN` value
```{r}
dt_numeric_ad <- as.numeric(dt1501_ad)
length(dt_numeric_ad[is.na(dt_numeric_ad)])
```
Number of numerical value after converting.
```{r}
length(dt_numeric_ad[!is.na(dt_numeric_ad)])
```
We back to the original before converting to seen why the converting failed for some cases.

```{r}
dt1501_ad[is.na(dt_numeric_ad)]
```
It seems that the un-convertable values are mostly missing values. Other cases are typos which we will remove.

In conclusion, for the case of `AD`, we convert the column to numeric and remove `NaN` value.

```{r}
dt1501_ad <- dt1501[, .(TUẦN, Ngày.tháng.năm, Ca, Mixer, Code.dầu, KL.theo.mixer, KL.thực.tế, Nước.rework, Batch, AD)]
dt1501_ad[, AD := as.numeric(AD)]
dt1501_ad <- dt1501_ad[!is.na(AD), ]
head(dt1501_ad)
```
Number of records in the current dataset

```{r}
nrow(dt1501_ad)
```
Column `KL.theo.mixer` is a constant depends on which mixer is refered to. Therefore, in this case, all of the value in column `KL.theo.mixer` should be 16500.

```{r}
print("Removing column KL.theo.mixer")
dt1501_ad <- dt1501_ad[, KL.theo.mixer := NULL]
```
Besides, the column `KL.thực.tế` should be calculated from the times series data of Weight of main mixer. Therefore, in the dataset of label, we remove it as well.

```{r}
print("Removing column KL.thực.tế")
dt1501_ad <- dt1501_ad[, KL.thực.tế := NULL]
```
Column `Nước.rework` contains information about how much rework water is used in batches. Unit of measurement is `kg`.

```{r}
as.numeric(dt1501_ad$Nước.rework)
```
We are not sure whether we will use this information. Better convert and save it.

```{r}
dt1501_ad[, Nước.rework := as.numeric(Nước.rework)]
```
We take a glance at the current status for the dataset

```{r}
summary(dt1501_ad)
```
Before saving the dataset, it is a must to check if the format for column `Batch` is correct and matched with column `Ngày.tháng.năm` and `Ca`.

Number of value in column `Batch` is in correct format
```{r}
sum(stringi::stri_detect_regex(dt1501_ad$Batch, pattern = "[0-9]{6}[Mm][0-9]{3}"))
```
It means all the value for this column are ready to use

```{r}
sum(stringi::stri_detect_regex(dt1501_ad$Batch, pattern = "[0-9]{6}[Mm][0-9]{3}")) == nrow(dt1501_ad)
```
For consistency, we convert all the lowercase `m` to uppercase.

```{r}
print("Uppercasing..")
dt1501_ad[, Batch := stringi::stri_trans_toupper(Batch)]
```
We next validate the matching between `Ngày.tháng.năm` and `Ca`.

The value in column `Ngày.tháng.năm` unfortunately can not be used since user save Date in unconsistent format and can not be transformed.

Therefore, we accept that `Batch` is the only time indicator here

```{r}
colnames(dt1501_ad)
dt1501_ad <- dt1501_ad[, .(Code.dầu, Nước.rework, Batch, AD)]
head(dt1501_ad)
```
Finally, we re-name the columns
```{r}
colnames(dt1501_ad) <- c("sku", "amount_rework", "batch", "label")
```

#### Summary

In conclusion, for criteria AD, we have already done these following processing step:

1. Fill NA Value in column target (AD).
2. Remove columns `KL.theo.mixer` and `KL.thực.tế`
3. Convert `Nước.rework` to numeric value
4. Re-format column `Batch` to uppercase
5. Drop other time indicators, only keep `Batch`
6. Rename the dataset

```{r}
data.table::setcolorder(dt1501_ad, c("label", "batch", "sku", "amount_rework"))
qs::qsave(x = dt1501_ad, file = "data/dt_label_ad.qs")
print("Saving done.")
```

### Vis

We follow the same approach above for `Vis`

Initiate dataset for `Vis`

```{r}
dt1501_vis <- dt1501[, .(Vis, Batch, Code.dầu, Nước.rework)]
head(dt1501_vis)
nrow(dt1501_vis)
```
Converting column Vis to numeric and see if there are invalid value

```{r}
dt1501_vis[is.na(as.numeric(dt1501_vis$Vis)), ]
```
We see that the values that are not convertable are NA value. Therefore, these data points are dropped from the dataset

```{r}
dt1501_vis[, Vis := as.numeric(Vis)]
dt1501_vis <- dt1501_vis[!is.na(Vis), ]
paste("Remanining data points", nrow(dt1501_vis))
```
Next, we convert `Nước.rework` to numeric as value. But in this case, we don't need to drop the NA value

```{r}
dt1501_vis[, Nước.rework := as.numeric(Nước.rework)]
summary(dt1501_vis)
```
After summarise the data table, we can see that the min value for Vis is `3.18`, which is not possible so may be this is an outlier.

Starting with 200 as the lower limit

```{r}
dt1501_vis[Vis < 200, ]
```

Next, we use 500, which is half of the first quarter

```{r}
dt1501_vis[Vis < 500, ]
```

There are lots of data points whose Vis under 500 so we can lower the setpoint to 300

```{r}
print("Which SKUs have value of Vis under 300")
unique(dt1501_vis[Vis < 300, Code.dầu])
```
Naturally, we calculate the proportion of batches belong to above SKU that have Vis value under 300

```{r}
lapply(unique(dt1501_vis[Vis < 300, Code.dầu]), function(val_sku) {
  paste(val_sku, ":", nrow(dt1501_vis[Vis < 300 & Code.dầu == val_sku, ]) / nrow(dt1501_vis[Code.dầu == val_sku, ]) * 100, "%")
})
```
Those SKUs that have vis-under-300 proportion smaller than `2%` are considered outlier and, hence, these data points will be removed

```{r}
l_sku_vis_under_300_outlier <- lapply(unique(dt1501_vis[Vis < 300, Code.dầu]), function(val_sku) {
  val_prop <- nrow(dt1501_vis[Vis < 300 & Code.dầu == val_sku, ]) / nrow(dt1501_vis[Code.dầu == val_sku, ]) * 100
  if (val_prop < 2) {
    return(val_sku)
  } else {
    return(NULL)
  }
})
l_sku_vis_under_300_outlier <- unlist(l_sku_vis_under_300_outlier)
l_sku_vis_under_300_outlier <- l_sku_vis_under_300_outlier[!is.null(l_sku_vis_under_300_outlier)]
print(l_sku_vis_under_300_outlier)
```
Removing these data points whose vis under 300 and belong to above SKU
```{r}
dt1501_vis <- dt1501_vis[!(Vis < 300 & Code.dầu %in% (l_sku_vis_under_300_outlier)), ]
paste("Remaining", nrow(dt1501_vis))
```

Just like `AD`, `Batch` will be checked whether it compromises with the pattern

```{r}
print("Number of record following pattern")
sum(stringi::stri_detect_regex(dt1501_vis$Batch, patter = "[0-9]{6}[Mm][0-9]{3}"))
paste("Over", nrow(dt1501_vis))
```
For consistency, we convert all the lowercase `m` to uppercase.

```{r}
print("Uppercasing..")
dt1501_vis[, Batch := stringi::stri_trans_toupper(Batch)]
```
Rename the columns as with `AD`

```{r}
colnames(dt1501_vis) <- c("label", "batch", "sku", "amount_rework")
head(dt1501_vis)
```
Finally, we save the dataset

```{r}
qs::qsave(dt1501_vis, "data/dt_label_vis.qs")
print("Saving done.")
```