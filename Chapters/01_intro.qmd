```{r}
Sys.setenv(QUARTO_PYTHON = "/Users/ducphu/miniconda3/envs/thesis1/bin/python")
Sys.setenv(RETICULATE_PYTHON = "/Users/ducphu/miniconda3/envs/thesis1/bin/python")
```

# Introduction {#sec-introduction}

In recent years, the raising of machine learning and deep learning has been kept at the highest level. The same is true for the attention they attracted from the research community. The introduction of Large Language Models is still one of the hottest topics around the world. However, alongside the influence deep learning has brought, its application in manufacturing and heavy industries is still questionable. In a field that requires lots of confidence and scientific proof before making any decisions, the capability of artificial intelligence, in general, to provide an explanatory suggestion is challenged. Its low interpretability even makes the discouragement bigger regarding deploying an intelligence system in these areas. Moreover, not only does the model architecture hardly persuade the business owner to believe its capacity, but only the features used to build and be inferred by the models are required to be understandable and aligned with the domain expertise. However, despite the trend in industries to reduce the influence of human workers on the shop floor, as stated in [@rahman_machine_2023], the gap preventing the integration of machine learning models is the difficulty in training the human resources to be able to control the intelligence system, [@md_review_2022]. This leads to the uprising attention from researchers for the concept of explainable artificial intelligence [@barredo_arrieta_explainable_2020].

The literature in the field of designing transparent models and implementing a method to interpret models' behaviors has made a big step in recent years. The first approach is to choose a transparent model at first so that the explainability is ensured from the beginning [@barredo_arrieta_explainable_2020; @angelov_explainable_2021]. The other way is to focus on the performance and then come up with a solution to explain the predictions of the models [@hassija_interpreting_2024].

This study is dedicated to contributing to the shared knowledge of both applying machine learning in industries and enhancing the ability of humans to understand these machine learning models. The chemical engineering industry or home care liquid manufacturing to be more specific, is the main subject of this research. The goal is to develop an intelligence system that is ready to be deployed and solves one of the key problems for the industry. The discussed issue is to predict the quality of the semi-products, which can help operators detect the potential failed batches early and make an adjustment based on the influence of features on the prediction. The main contributions delivered in our research are:

1. We construct a customized preprocessing for the time series extracted from devices and instruments in the processing line, including the transformation of the label set, and the cleaning and engineering of features.
2. We evaluate two machine learning models belonging to two different categories in the Explainable Artificial Intelligence field. The criteria for evaluation are performance and the explainability of feature importance for each model. The results consist two models for predicting products' pH and viscosity, whose the R-squared are about $71.7\%$ and $89.4\%$, respectively.
3. We experiment with a feature selection mechanism based on SHaley Additives exPlantion (SHAP) which is an interpretable computation that indicates the influence of models on the predictions.