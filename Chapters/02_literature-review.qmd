# Literature Review {#sec-literature_review}

In a world where technology, especially Artificial Intelligence (AI) is blooming. Finding a way to implement advanced algorithms and research into normal life is a key challenge and a steel-hot topic. Manufacturing is a multi-billion industry that plays a central role in supplying and distributing goods to people. However, since the price for making mistakes in producing products for lives is much more serious than in the industry of entertainment or art, people in charge of corporations tend to be more careful when applying AI in their business, especially when every prediction comes with a failure chance. Managers require an understandable explanation for models whose behaviors should be aligned with industry domain knowledge. Therefore, although Deep learning (DL) is growing fast, AI applications in the industry are still more of an opportunity for researchers. In this chapter, we review the literature around the implementation of machine learning (ML) in industry and the drawbacks that prevent AI from taking over the highly-valued domain.

## Machine learning in Industry {#sec-ml-industry}

Industries' profit depends on the flow of materials from suppliers to customers to be sustainable, low-risk, low-cost, and responsive. Business head requires as least human errors as possible to maintain the stability of companies. As stated in [@rahman_machine_2023], the necessity of lowering human dependency in manufacturing is increasing. Hence, using technology that can automatically make decisions or support the process of making operation decisions is the place where AI can be brought into play. However, the opinions of experts in a specified domain still have a bigger weight in the decision-making process. The circumstances where all the operations are decided by machine learning are still fictional. Another aspect that needs to be mentioned is the responsibility of decision-makers. Machine learning systems cannot take responsibility in place of humans.  [@angelopoulos_tackling_2019] discusses how the framework of the human-machine interaction system can reduce the fatality of human errors while maintaining the criticality of the expert factor. Actions made from ML-generated insights without many significant changes to the process are the reason why factories are now interested in implementing AI, [@rai_machine_2021].

The power of ML can be seen in many aspects such as turning the current transactional databases into insight by using the analytical power of ML techniques and the adaptability to unexpected circumstances [@wuest_machine_2016]. Dimensionality curse is less of a problem with the introduction of ML algorithms like support vector machines (SVM) [@wuest_machine_2016]. The problem-solving skill of ML is proved by learning human-like behaviors and strategies such as autonomous driving cars and chess-like games, which leads to the replacement or defeat of humans in those areas [@bertolini_machine_2021]. Moreover, the achievements in DL also allow research to solve multiple problems with the dataset. Noisy data can be treated by autoencoders and limited data can be generated by generative AI. A wide range of studies and approaches helps popularize the techniques for AI engineers to get rid of long-lasting issues [@bertolini_machine_2021].

With the advantages discussed above in addition to the growth of the Internet of Things (IoT), many and many industries now try to include AI in their manufacturing process. As listed in [@rahman_machine_2023], the agricultural and mining field and power industry have joined in the trend. [@ameri_sianaki_machine_2019] surveys ML applications found in healthcare, smart electrical management, smart cities, and green supply chain management. E-commerce is also mentioned in [@sarker_machine_2021] with the application to analyze customer behavior and product recommendations. Research is reviewed by [@bertolini_machine_2021] about four domains that apply ML increasingly, namely maintenance management, quality management, production planning and control, and supply chain management. These are examples where intelligence systems are helping humans to improve business performance. Different domains may have different kinds of problems. The reason why AI is used in many distinct industries is its ability to solve multiple issues.

According to [@rahman_machine_2023], AI can be implemented in many industries to solve a variety of problems, such as failure prediction, smart quality assurance, automation in inventory management, etc. [@rai_machine_2021] surveys computer-vision-based inspection and monitoring, as well as fault detection and process improvement. The same conclusion can be reached from [@bertolini_machine_2021; @wuest_machine_2016], where image recognition and many ML techniques are deployed to solve the problem of defect detection. Quality assurance (QA) or quality control (QC) is always an important key performance indicator for every manufacturing factory. Online quality control research which takes sensor data as inputs is found by [@angelopoulos_tackling_2019; @ameri_sianaki_machine_2019]. Computer-vision-based inspection is among the most common topics when it comes to QC [@rahman_machine_2023; @sarker_machine_2021]. In a comprehensive survey about ML in QC, [@sarker_machine_2021] notices the imbalance in the type of process. Physical processes such as cutting receive more attention from researchers than the changes in material properties. Most of the studies in [@md_review_2022] are about QA in the assembly process.

On the other hand, supply chain management with topics like demand forecasting and inventory management are tackled by different types of techniques and points of view, in [@bertolini_machine_2021; @ameri_sianaki_machine_2019; @rai_machine_2021]. The hospitability industry witnessed the arrival of intelligence systems in [@rangsit_university_thailand_artificial_2023], which impacts both the current workflow job opportunities. The sustainability of the petroleum industry is a newly tackled issue for ML, in addition to the traditional problem of improving yields [@rahmanifard_application_2019]. These scientific works provide a strong foundation to claim the undeniable benefit that ML has brought to manufacturing.

## Predict the quality of home care product

The popularity of ML in QA is proved in [@sec-ml-industry]. However, most of the studies focus on QC in integrated circuit manufacturing, mining, or agriculture. Very little attention from the research society is given to the chemical industry. The chemical industry has been a highly valuable market for a long time. Many categories fall in the domain of chemical products, such as inks, cleaning compounds, agricultural chemicals, etc. Hence, applying ML is expected to be the front-runner for improving efficiency and quality assurance in chemical manufacturing. Therefore, we studied the research made on topics of ML integration in the chemical industry.

The very first challenge on the table for ML is to make better decisions in the process of chemical engineering. [@schweidtmann_machine_2021] discusses the importance of choosing the right path based on the existing data and domain knowledge. The characteristics of chemical reactions are complex and dangerous if it is not put under close control. The traditional approach which is developing a physicochemical model is too expensive and many of the interactions between chemicals cannot be reproduced by computationally tractable models, [@schweidtmann_machine_2021]. Additionally, chemical companies are increasingly spending resources to establish data science teams whose objectives are to fuel breakthroughs in maintenance monitoring, yield, and energy optimization, as discussed in [@mowbray_industrial_2022]. 


[@mowbray_industrial_2022] notices the astonishing drawbacks of literature focusing on introducing ML to chemical process engineering. Missing domain knowledge while integrating ML techniques may lead to obvious or misleading insights. In terms of building a data team for chemical engineering, [@mowbray_industrial_2022] also states that the approach of teaching process engineers ML algorithms is much more efficient than training data scientists in the technical aspects of the chemical industry. These findings lead to a novel approach by combining the principles in processing engineering with the knowledge of ML that can reduce each other weaknesses [@mowbray_industrial_2022].

The lack of interpretability for black-box models is among other challenges the data scientists have to solve when building a decision-making supporting intelligence system in the industry of chemical manufacturing [@dobbelaere_machine_2021]. Being on the same side as [@mowbray_industrial_2022], [@dobbelaere_machine_2021] also agrees that the distance between AI and chemical engineering is the key factor preventing the growth of ML in this considered domain. The black-box nature brings about the possibility of overfitting models that are not easily detected. The concept of "garbage-in-garbage-out" discussed in [@dobbelaere_machine_2021] increases the requirement of explainable feature engineering and feature selection, where the fundamental backgrounds of chemical engineering are applied to double-check the meaningfulness of factors.

Drawbacks usually bring opportunities. To solve the issues that most ML models are still facing, research communities are putting efforts into many approaches to increase the interpretability of models, such as domain-based feature selection, explainable feature engineering transparent models, etc. In the next section, we discuss the definition of explainable artificial intelligence, the current situation, and the pros and cons of such techniques. 

## Application of Explainable AI {#sec-explainable-ai}

ML is supported intensively. Therefore, the number of ML architectures has rapidly growth over the years. [@tercan_machine_2022] discovers a wide range of techniques like regression analysis for detecting linear and quadratics correlation, support vector machine (SVM), and feed-forward neural network for non-linear estimations. Ensemble and tree-based models were also found in [@tercan_machine_2022] with a large amount of research.  K-nearest neighbor, naive Bayes, and logistic regression are added by [@md_review_2022] to the set of ML algorithms in Industry 4.0. Besides ML and deep learning (DL), dimensionality reduction and feature learning cannot be left out when developing an end-to-end ML solution for industries. Feature selection using ANOVA and feature extraction with PCA are the top common techniques discussed in [@sarker_machine_2021].

However, AI is not a jack-of-all-trade tool. Many drawbacks prevent ML from successfully deployed in the manufacturing process can be named. [@md_review_2022] discusses the difficulty of integrating ML into a human-machine system for quality prediction as the complexity of the ML algorithm creates a gap in human development which is required to enable the interaction between operators and intelligence systems. [@sarker_machine_2021] states in the research that the nature and characteristics of data determine the overall performance and usage of ML, which raises a concern about whether models only work on the training data but fail when facing real-life issues. Besides appropriate feature selection and model selection for specific challenges and data, [@wuest_machine_2016] notices that the interpretation of results is a problem for ML algorithms. The purpose of [@fujii_guidelines_2020] in making a guideline for ML implementation to ensure the quality of models is that systems behave based on data and not from logic design as a black box invalidates many principles in software development. Advanced ML techniques still face acceptability issues from users since their inferences are considered unintuitive and incomprehensible to humans, [@islam_systematic_2022]. In this section, we discuss the definition of explainable artificial intelligence (XAI) and research related to it.

Begin with the concept of XAI, [@barredo_arrieta_explainable_2020] lists *understandability*, *comprehensibility*, *interpretability*, *explainability*, and *transparency* are all key concepts, with *understandability*, which means the ability for human to realize how the ML models work without explicitly discussing the algorithm, ranking the highest important one. Also in [@barredo_arrieta_explainable_2020], three main objectives for a model to be considered explainable are

- To detect and correct the bias in the training dataset.
- To ensure models' robustness by highlighting the factors that can impact models' predictions.
- To allow only causal variables to be used as inputs.

In [@hassija_interpreting_2024], Hassija et. al. discuss a different point of view on the need that XAI should provide, such as

- To provide reasoning for their predictions, rather than the computation and algorithm logic they hold.
- To be more resilient from the adversarial perturbation resulting from the training dataset.
- To evolve with the insights given from the model explanation.
- To ensure the making of fair and ethical decisions.

*Transparency*, *Interpretability*, and *Explainability* are mentioned when Angelov et al. discuss the terms related to XAI in [@angelov_explainable_2021]. The more transparent the models are, the higher the potential of humans to understand their behaviors of 
them, via a explainable interface between users and intelligence systems.

To achieve the status of being explainable, many different approaches are introduced, splitting into different categories. The methods of XAI are based on the *Stage*, the *Scope* of the models. Islam et al. re-introduced the branches of XAI depending on when the explanations are made and the extent of them in [@islam_systematic_2022]. Barredo et al. and Angelov et al. share the same idea with the division of post hoc explanation and the transparent models that are explainable-by-design in [@barredo_arrieta_explainable_2020; @angelov_explainable_2021]. Barredo went further to introduce three levels of model transparency, including *algorithmic transparency*, *decomposability*, and *simulatability*. Meanwhile, the post hoc explanations are diverse with different formats, such as text, visual explanation, local explanation and feature relevance explanation techniques [@barredo_arrieta_explainable_2020]. Angelov et al. also cared about the subject of the method, focusing on the features or the parameters of models[@angelov_explainable_2021].

Among the transparent models or so-called ante hoc interpretability that are understandable by humans, Linear Regression (LR) raises a bit of concern [@barredo_arrieta_explainable_2020] as its nature depends on the domain knowledge that explains the linear dependence between the factors and the outcome. In [@hassija_interpreting_2024], LR is also categorized as an understandable model architecture, along with decision trees, and k-nearest neighbors (k-NN). Another problem with linear regression is that its ability to be decomposable depends heavily on the size of the models, which is the number of factors the models try to evaluate the impact on outcomes [@barredo_arrieta_explainable_2020]. The dependable variables also need to be understandable themselves, otherwise, it will reduce the overall transparency of the model.

The concept of k-NN is clear enough for users to understand since it predicts based on the computation of the distances to points in the training data set using a *voting* system for categorical output and *aggregation* system for the continuous variant. Though k-NN resembles the behavior of humans regarding treating new issues based on the experience in the past, the choices of *k* are sometimes fuzzy to the users. The same is correct for using a more complex distance function [@barredo_arrieta_explainable_2020]. Hassija et al. agree with the point of view that k-NN is easier to understand but the performance is usually lower compared to other complex models [@hassija_interpreting_2024].

The trade-off between the performance or accuracy of models and their understandability is highly agreed across scientific communities [@barredo_arrieta_explainable_2020, @hassija_interpreting_2024, @islam_systematic_2022]. However, the demand for an efficient algorithm which can also be explained has never decreased. Therefore, effort has been made to improve the interpretability of ML architecture whose performances rank high among others, such as deep learning or ensemble methods. That brings us to the post hoc techniques which aim at providing a better understanding of the predictions of an already built model and its inputs.

The two main categories of this technique are model-agnostic which applies to various models, and model-specific designed for a specific architecture. In the field of the *Model agostic* approach, *visualization*, *prototypes and criticisms*, *counterfactuals*, etc. are surveyed by Hassija et al. as sub-categories in [@hassija_interpreting_2024]. Meanwhile, Barredo et al. go with a slightly different point of view in which the most common techniques for model agostic explanation are *model simplification*, *feature relevance* estimation and *visualization* as well [@barredo_arrieta_explainable_2020].

From a different aspect, Zacharias et al. argue that the importance of an explainable feature selection does not receive enough attention in [@zacharias_designing_2022]. Three different problems related to the process of feature selection are stated: Feature selection methods reduce the impacts to one value only. Eliminating features affects the performance of models. Moreover, XAI puts very little attention on users [@zacharias_designing_2022]. From the scientific work gathered, Zacharias et al. found evidence that a model weighing all input variables equally across predictions is not comprehensive. The global property of feature importance is also not suitable according to [@zacharias_designing_2022]. However, if the feature selection is too strict, a reduced training set may lead to decreased performance as well, which enhances the need for capable feature selection methods. The research suggests a highly SHAP-influenced system that solves the issue of explainable feature selection.

## SHapley Additive exPlanation {#sec-shap}

In the previous chapter, we discussed how the scientific community approaches the XAI. Besides the transparency of models themselves, one of the most common techniques is using the SHapley Additive exPlanations (SHAP)[@lunberg_shap_2017] in both the pre-modeling explanation and post hoc interpretability for ML model [@barredo_arrieta_explainable_2020; @hassija_interpreting_2024; @islam_systematic_2022; @zacharias_designing_2022]. This chapter is dedicated to reviewing the literature around the implementation of SHAP in the preprocessing of data such as feature selection and transforming model predictions to human-understandable information.

SHAP was first introduced by Lundberg and Lee in [@lunberg_shap_2017] to solve the problem of the interpretability for ensemble and deep learning models. From that point, SHAP received loads of attention from researchers who started to apply it in different phases of constructing an intelligence solution. Three main concepts of SHAP are [@lunberg_shap_2017]: SHAP unifies six different *additive feature attribution* methods in explaining model predictions. SHAP value is guaranteed the performance by game theory. SHAP is aligned with human intuition.

The productiveness of SHAP is proven through many papers implementing it in different industries. Nohara et al. interpret a gradient-boosting decision tree in an intelligence system for hospitals in [@nohara_explanation_2022]. A deep learning model, autoencoders, implemented for abnormal detection, is also explained by Kernel SHAP in [@antwarg_explaining_2021]. Sahlaoui et al. proposed a new ensemble tree-based algorithm for mining the insights from student performance data, along with an XGBoost model. These two models are then interpreted by SHAP to provide a better understanding [@sahlaoui_predicting_2021]. Despite the mathematical and human-centric problem SHAP faces in [@kumar_problems_2020] by Kumar et al, the number of works that integrate SHAP as a method of increasing interpretability grows fast as its ability to be applied to different types of ML algorithms. In [@lunberg_shap_2017], Kernel SHAP is introduced as a model-agonistic approach for ensemble models and other ML architectures in general. Linear SHAP and Low-order SHAP are preferred when comes to the linear model. Max SHAP provides an additional option. Meanwhile Deep SHAP is designed specifically for deep-learning models. To be more persuasive, the advantage in computation speed [@lunberg_shap_2017] also helps to bring up the popularity of SHAP.

Besides the designed purpose of interpreting ML models, SHAP is also used in the phases before the birth of an intelligence solution. We have discussed the work of Zacharias et al. in [@zacharias_designing_2022] in terms of proposing a SHAP-based feature selection technique. To reduce the dimension of the problem, feature engineering techniques such as Principle Component Analysis (PCA), Uniform Manifold Approximation and Projection (UMAP) are usually performed before training an ML model. However, the nature of introducing new features with non-linear equations may decrease the interpretability of the inputs, as well as the model itself [@marcilio_explanations_2020]. As the interest in constructing understandable models, explainable feature selection techniques, some of which are based on SHAP and Shapley Additive Global importancE (SAGE) value, is the topic discussed in [@marcilio_explanations_2020; @fryer_shapley_2021]. The general idea of applying a post hoc technique like SHAP is to evaluate the importance of features based on model predictions, and then, to re-select the subset of features that is more suitable [@fryer_shapley_2021]. The results from Fry et al. in [@fryer_shapley_2021] show that in the range of keeping $42\%$ to $80\%$ of the original feature set, the approach using TreeSHAP provides better accuracy, compared with ANOVA, Mutual Information (MI), and Recursive Feature Elimination (RFE).

To sum up, in this chapter, we have discussed the status of machine learning in industries, especially in chemical engineering. An undeniable interest in the field of explainable artificial intelligence has grown in recent years as well, especially with the introduction of SHAP. In the scope of this study, we conduct experiments of *implementing explainable artificial intelligence in predicting the quality of home care liquid products*. A transparent model, Linear Regression, is chosen as a baseline model, compared with a more complex yet efficient ensemble model, Random Forest, in the task of regression. Feature selection mechanism using SHAP is also implemented and discussed, along with the designated purpose of interpreting model predictions of SHAP.