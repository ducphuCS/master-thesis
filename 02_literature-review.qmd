# Literature Review {#sec-literature_review}

## Improving performances with statistic-based feature selection

We have total of 53 dataset, over 40 of them are data from machine, which results in many features can be extracted.

To enhance the performance of models, we conduct statistic analysis to determine the impact of features on the outcome defined above

For example, how the average of temperature in the batch has correlation with the pH value.

Regarding the statistic analysis we mentioned, some approaches can be:

- ***Hypothesis testing*** to accept or reject the effect of factors on the outcome
- **Analysis of correlation**  to select features 

According to @CHANDRASHEKAR201416, there are 3 main types of feature selection methods `Filter`, `Wrapper`, and `Embedded`.

- **Filter** use variable ranking techniques for variable selection by ordering.
- **Wrapper** use predictor as black box and the performances as the objective function to evaluate the variable subset.
- **Embedded** incorporate the feature selection as part of the training process.

### Using Filter

`Hypothesis testing` and `analysis of correlation` use $p\_value$ and $corr$, respectively, as the criteria for ranking. Therefore, both of them belongs to **filter** class.

`Mutual information` (MI) and other methods estimating the MI are also candiates as a principle criteria, such as the measure $K$ of the Kullback-Leibler divergence or Conditional Mutual Information

One of the drawbacks of ranking methods is that they do not eliminate redundant variables in [@guyon2003introduction]. Also, features may be less effective when standing alone but have impact on the target when in form of combination with others in [@guyon2003introduction; @xu2010discriminative].

### Using Wrapper

In [@CHANDRASHEKAR201416], Wrapper can be splitted to two main categories, namely `Sequential Selection Algorithms` and `Heuristic Search Algorithms`.

The main drawbacks for Wrapper methods is that they require computations to select optimal set of features. By using classifiers and their performances as scoring criteria, Wrapper is also vulnerable with model overfitting in [@KOHAVI1997273].

### Using Embedded

Embedded methods want to reduce the computations used by Wrapper by incorporating the feature selection in the training process. Also by combining the ranking criteria as MI and the classificatin models, Embedded methods can limit somes of the cons of Filter approaches.

In [@battiti1994using; @Kwak2002; @peng2005feature], MI is used with other classifiers to acquire the feature subset. Using weights of a classification model as a ranking table to remove feature is also widely used, in [@guyon2003introduction; @guyon2002gene].

### Others

Ensemble feature selection in [@haury2011influence; @abeel2010robust] is a novel candiate that they conduct single feature selection on a bootstrap data sample and aggregate to a final feature set.

## Explainable AI

Artificially intelligence system increasingly affects human, therefore, human demands a clearer explainations from these system before making decisions. 

According to [@app12031353], the concepts of explainable artificial intelligence (XAI) have four main aspects, namely `Stage`, `Scope`, `Input` and `Output`, in [@vilone2021]. 

### Stage

[@vilone2021] states that the `stages` can be defined as *ante hoc* and *post hoc*.

- ***Ante hoc*** starts before the training of data to generate explaination for the decisions. For examples, transparent models such as `fuzzy models` and `tree-based models`
- ***Post hoc*** combines the base model with external model which mimics the base model's behaviour to generate an explaination for the users. For examples, this method is usually associated with models in which the inference process is a blackbox, such as `support vector machines` or `neural networks`.

### Scope

The scope defines the extent of an explaination, globally or locally. Global scope means the whole inferential technique of a model is made transparent to the user. Meanwhile, explaination which is given to user for a single instance of inference is called local scope. The `decision tree` can be an example for both scope. The whole tree represents the global scope, while a single branch ca be treated as a local explaination.
